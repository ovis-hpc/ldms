.\" Manpage for LDMS_QuickStart
.\" Contact ovis-help@ca.sandia.gov to correct errors or typos.
.TH man 7 "10 Nov 2015" "v3" "LDMS_QuickStart man page"

.SH NAME
LDMS_QuickStart - man page for Quick Start of LDMS

.SH INTRODUCTION
LDMS is the Lightweight Distributed Metric Service. LDMS is a distributed data collection, transport, and storage tool that supports a wide variety of configuration options.
There are three main functional components described below.
.PP
.I
Samplers
run one or more plugins that periodically sample data of interest.
Each plugin defines a group of metrics called a metric set.
The sampling frequency is user defined and can be dynamically changed.
A host can simultaneously run multiple plugins.
Configuration flags determine whether the sampling plugins run synchronously or asynchonously
(both on a host and across hosts). Memory allocated for a particular metric set is overwritten by each
successive sampling. The host daemon does not retain sample history;
plugins do not typically retain history, but can be written to do so.
.PP
.I
Aggregators
collect data in a pull fashion from samplers
and/or other aggregators. The collection frequency
is user defined and operates independently of other
collection operations and sampling operations. Distinct metric
sets can be collected at different frequencies. Once started, the aggregation schedule cannot
be altered without restarting the aggregator. Fan-in refers to
the number of hosts collected from by a single aggregator.
Maximum fan-in varies by transport but is roughly
9,000:1 for the socket transport and for the RDMA
transport over Infiniband. It is > 15000:1 for RDMA over
the Cray Gemini transport. Daisy chaining  is not limited to two levels;
multiple aggregators may aggregate from the same sampler or aggregator ldmsd.
Fan-in at higher levels is limited
by the aggregator host capabilities (CPU, memory, network
bandwidth, and storage bandwidth).
.PP
.I
Storage
plugins write in a variety of formats.
Comma Separated Value (CSV) file storage of metric sets
plugins are provided. Storage occurs when a
valid updated metric set data is collected by an aggregator that
has been configured to write that data to storage. Collection of
a metric set whose data has not been updated or is incomplete
does not result in a write to storage in any format. The store_derived_csv plugin
can be configured to retain the immediately previous state history and thus store
rate values.

.PP
The host daemon is the same base code in all cases; differentiation is based on configuration of plugins for sampling or
storage and on configuring aggregation of data from other host daemons.


.SH DESCRIPTION
Quick Start instructions for LDMS (Lightweight Distributed Metric Service).
.PP
This man page describes how to configure and run LDMS daemons (ldmsd) to perform the following tasks:
.IP \[bu]
collect data
.IP \[bu]
aggregate data from multiple ldmsds
.IP \[bu]
store collected data to files.
.PP
There are four basic configurations that will be addressed:
.IP \[bu]
configuring ldmsd
.IP \[bu]
configuring a collector plugin on a running ldmsd
.IP \[bu]
configuring a ldmsd to aggregate information from other ldmsds
.IP \[bu]
configuring a store_csv storage plugin on an ldmsd.
.PP
The order in which these configurations should be performed does not matter with respect to collectors and aggregators.
.PP
While a complete listing of flags and parameters can be seen by running
ldmsd and the configuration tools with the --help directive, this document describes the flags and parameters required for running a basic setup.
.PP
There are no run scripts provided in the current release; the examples here can be used in the creation of such.

.SH PREREQUISITES:
.PP
.IP \[bu]
All sections below assume the build directory is /tmp/opt/ovis.
.IP \[bu]
libevent-2.0 is a requirement.
.IP \[bu]
Python 2.7 or Python 2.6 with the argparse module is required for ldmsd_controller

.SH BUILD/INSTALL:
.PP
There is a separate document with build/install instructions.
.PP
The default ldms build in v3 include authentication turned on.
This document does not include use of the authentication flags; the instructions here
are as if you had built with --disable-auth. For more information on authentication,
see the ldms_authentication man page.

.SH CONFIGURATION TOOLS:
.PP
ldmsd_controller is the current preferred configuration tool for v3.
A configuration file interface is in development.
ldmsctl which was the configuration tool for v2 will still work, however.

Information on these tools and their arguments can be found in the appropriate man pages.

Both ldmsd_controller and ldmsctl have both command line executables that can work in interactive mode
and can be directed commands/scripts to a socket. In order to get support for the interactive mode,
ldms must be built with
.B --enable-readline.
In many cases, the command/script redirection is the way people choose to configure. In that case
you may build with --disable-readline.
.PP
In the instructions below, we first illustrate use of ldmsctl vs ldmsd_controller
and the use of the interactive interface vs a script interface.
In all subsequent sections, we consider only the syntax for using ldmsd_controller
as the configuration tool, with it called from within an encompassing script.


.SH START AN LDMSD ON YOUR HOST
Currently must be run be run as root if using the default path of /var/run/ for the unix domain socket. This can be changed using the environment variable LDMSD_SOCKPATH as described below.
.SS
Set environment variables:
.nf
.RS
export LD_LIBRARY_PATH=/tmp/opt/ovis/lib/:/tmp/opt/ovis/lib/ovis-ldms/:<path to libevent-2.0>/lib:$LD_LIBRARY_PATH
export ZAP_LIBPATH=/tmp/opt/ovis/lib/ovis-ldms
export LDMSD_PLUGIN_LIBPATH=/tmp/opt/ovis/lib/ovis-ldms
export PATH=/tmp/opt/ovis/sbin/:/tmp/opt/ovis/bin:$PATH
export LDMSD_SOCKPATH=/tmp/run/ldmsd  #(non-root. Change -S arguments accordingly)
.RE
.fi


.SS
Run ldmsd:
.nf
.RS
<path to executable>/ldmsd -x <transport>:<listen port> -S <unix domain socket path/name> -l <log file path/name> -v <LOG_LEVEL>
.RE
.fi

Notes:
.IP \[bu]
Transport is one of: sock, rdma, ugni (ugni is Cray specific for using RDMA over the Gemini/Aries network)
.IP \[bu]
The unix domain socket is used by ldmsctl to communicate configuration information to an ldmsd.
The default path for this is /var/run/ldmsd/. To change this the environment variable LDMSD_SOCKPATH must be set to the desired path (e.g. export LDMSD_SOCKPATH=/tmp/run/ldmsd)
.IP \[bu]
No log can be can be obtained by specifying /dev/null for the log file or using command line redirection as shown below. Also LOG_LEVEL
QUITE produces only user-requested output.
.IP \[bu]
The default is to run as a background process but the -F flag can be specified for foreground
.IP \[bu]
A script can be made to start ldmsd and collectors on a host where that script contains the information to execute the command.

.PP
.SS Examples for running ldmsd:

.IP \[bu]
Start an ldmsd on the socket transport with a log file
.nf
.RS
/tmp/opt/ovis/sbin/ldmsd -x sock:60000 -S /var/run/ldmsd/metric_socket -l /tmp/opt/ovis/logs/1
.RE
.ni

.IP \[bu]
Same but sending stdout and stderr to /dev/null
.nf
.RS
/tmp/opt/ovis/sbin/ldmsd -x sock:60000 -S /var/run/ldmsd/metric_socket -l /tmp/opt/ovis/logs/1  > /dev/null 2>&1
.RE
.fi

.IP \[bu]
Start 2 instances of ldmsd on host vm1
.nf
.RS
Note: Make sure to use different socket names and listen on different ports if you are on the same host.
/tmp/opt/ovis/sbin/ldmsd -x sock:60000 -S /var/run/ldmsd/metric_socket_vm1_1 -l /tmp/opt/ovis/logs/vm_1  > /dev/null 2>&1
/tmp/opt/ovis/sbin/ldmsd -x sock:60001 -S /var/run/ldmsd/metric_socket_vm1_2 -l /tmp/opt/ovis/logs/vm_2  > /dev/null 2>&1
.RE
.fi

.SH CONFIGURE COLLECTORS ON A HOST
.SS Set environment variables
.nf
.RS
export LD_LIBRARY_PATH=/tmp/opt/ovis/lib/:/tmp/opt/ovis/lib/ovis-ldms/:<path to libevent-2.0>/lib:$LD_LIBRARY_PATH
export ZAP_LIBPATH=/tmp/opt/ovis/lib/ovis-ldms
export LDMSD_PLUGIN_LIBPATH=/tmp/opt/ovis/lib/ovis-ldms
export PATH=/tmp/opt/ovis/sbin/:/tmp/opt/ovis/bin:$PATH
export LDMSD_SOCKPATH=/tmp/run/ldmsd  #(non-root. Change -S arguments accordingly)
.RE
.ni

.SS CONFIGURE A COLLECTOR ON VM1 DIRECTLY VIA LDMSCTL (Option 1):

.SS Run ldmsctl:
.PP
.RS
ldmsctl -S <unix domain socket path/name associated with target ldmsd>
.RE
.br
.SS Example for running ldmsctl:
.nf
.RS
/tmp/opt/ovis/sbin/ldmsctl -S /var/run/ldmsd/metric_socket_vm1_1
ldmsctl>
.RE
.ni

.SS Configure a collector with ldmsctl
Now configure "meminfo" collector plugin to collect every second.
.nf
.RS
ldmsctl> load name=meminfo
ldmsctl> config name=meminfo producer=vm1_1 instance=vm1_1/meminfo
ldmsctl> start name=meminfo interval=1000000
ldmsctl> quit
.RE
.ni

.PP
Notes:
.IP \[bu]
At the ldmsctl> prompt typing "help" will print out info about the ldmsctl commands and options.
.IP \[bu]
interval=<# usec> e.g interval=1000000 defines a one second interval.
.IP \[bu]
You can use stop name=meminfo followed by start name=meminfo interval=xxx to change collection intervals.
.IP \[bu]
For synchronous operation include "offset=<#usec>" in start line (e.g. start name=meminfo interval=xxx offset=yyy).
This will cause the sampler to target interval + yyy aligned to the second and micro second
(e.g. every 5 seconds with an offset of 0 usec would ideally result in collections at 00:00:00, 00:00:05, 00:00:10, etc.
whereas with an offset of 100,000 usec it would be 00:00:00.1, 00:00:05.1, 00:00:10.1, etc)
.IP \[bu]
Different plugins may have additional configuration parameters. Use help within ldmsctl to see these.
.IP \[bu]
At the ldmsctl> prompt typing "info" will output all config information to that ldmsd's log file.
.PP

.SS Verifying the collector
.PP
At this point the ldmsd collector should be checked using the utility
.B ldms_ls
(See Using ldms_ls below)

.SS CONFIGURE A COLLECTOR ON VM1 USING A BASH SCRIPT (Option 2):
.PP
The following performs the same as the above (but with the addition of the vmstat plugin),
but using the command direction into the socket,
An example bash script named "collect.sh" then contains all the commands and performs the
direction into the socket:
.nf
.RS
#!/bin/bash
export LD_LIBRARY_PATH=/tmp/opt/ovis/lib64/:$LD_LIBRARY_PATH
export ZAP_LIBPATH=/tmp/opt/ovis/lib64/ovis-ldms
export LDMSD_PLUGIN_LIBPATH=/tmp/opt/ovis/lib64/ovis-ldms
# LDMSD_SOCKPATH for non-root. Change -S arguments accordingly.
export LDMSD_SOCKPATH=/tmp/run/ldmsd
LDMSCTL=/tmp/opt/ovis/sbin/ldmsctl

# Configure "meminfo" collector plugin to collect every second (1000000 usec) on vm1_2
echo load name=meminfo | $LDMSCTL -S /var/run/ldmsd/metric_socket_vm1_2
echo config name=meminfo producer=vm1_2 instance=vm1_2/meminfo | $LDMSCTL -S /var/run/ldmsd/metric_socket_vm1_2
echo start name=meminfo interval=1000000 | $LDMSCTL -S /var/run/ldmsd/metric_socket_vm1_2
# Configure "vmstat" collector plugin to collect every second (1000000 usec) on vm1_2
echo load name=vmstat | $LDMSCTL -S /var/run/ldmsd/metric_socket_vm1_2
echo config name=vmstat producer=vm1_2 instance=vm1_2/vmstat | $LDMSCTL -S /var/run/ldmsd/metric_socket_vm1_2
echo start name=vmstat interval=1000000 | $LDMSCTL -S /var/run/ldmsd/metric_socket_vm1_2
.RE
.fi
.PP
Make collect.sh executable
.RS
chmod +x collect.sh
.RE
.PP
Execute collect.sh (Note: When executing this across many nodes you would use pdsh to execute the script on all nodes in parallel)
.RS
./collect.sh
.RE
.PP
At this point the ldmsd collector should be checked using the utility
.B ldms_ls
(See Using ldms_ls below)

.SS CONFIGURE A COLLECTOR USING LDMSD_CONTROLLER (Option 3):
The syntax for configuring plugins via ldmsd_controller is the same as above,
ie.g., load, config, and start commands as above. However,
nlike ldmsctl, there is not a pipe to a socket option for the calls, rather
one must specify a script with the commands.

> more add_plugins.sh
.nf
.RS
#!/bin/bash
echo "load name=meminfo"
echo "config name=meminfo producer=vm1_2 instance=vm1_2/meminfo"
echo "start name=meminfo interval=1000000"
.RE
.ni

If one were to use the interactive interface for ldmsd_controller,
one could just specify the script when using the interface.
.nf
.RS
> ldmsd_controller --host vm1 --port=61002 --script='./add_plugin.sh'
.RE
.ni


If you don't want to use the interactive interface for ldmsd_controller,
you can invoke the ldmsd_controller and pass it the script from an encompassing
script, but including lines like:

.nf
.RS
cmd="ldmsd_controller --host vm1 --port=61002"
$cmd --script "<<path_to_script>>/add_prdcr.sh"
.RE
.ni

.PP
In all subsequent sections, we consider only the syntax for using ldmsd_controller
as the configuration tool, with it called from within an encompassing script.


.SH CONFIGURE AN AGGREGATOR USING LDMSD_CONTROLLER
.SS Start ldmsd's to collect
.PP
Start the 2 collector ldmsd's as described above.

.SS Set environment variables
.nf
.RS
export LD_LIBRARY_PATH=/tmp/opt/ovis/lib/:/tmp/opt/ovis/lib/ovis-ldms/:<path to libevent-2.0>/lib:$LD_LIBRARY_PATH
export ZAP_LIBPATH=/tmp/opt/ovis/lib/ovis-ldms
export LDMSD_PLUGIN_LIBPATH=/tmp/opt/ovis/lib/ovis-ldms
export PATH=/tmp/opt/ovis/sbin/:/tmp/opt/ovis/bin:$PATH
export LDMSD_SOCKPATH=/tmp/run/ldmsd  #(non-root. Change -S arguments accordingly)
.RE
.ni

.SS Start an ldmsd to aggregate
Start a ldmsd on your vm using "sock" as the listening transport
.RS
/tmp/opt/ovis/sbin/ldmsd -x sock:60002 -S /var/run/ldmsd/metric_socket_agg -l /tmp/opt/ovis/logs/vm1_agg -p 61002 > /dev/null 2>&1
.RE

.SS Write a script to add producers and start collecting from them using the ldmsd_controller configuration tool:
This adds vm1_1 as a producer with its sets collected at 2 second intervals
and vm1_2 as a producer with its sets collected at 5 second intervals. Here the "name" of the producer
must match the "producer" name given to the sampler.

The first set of lines adds the producers. The second set of lines establishes the aggregation from them.
at the specified intervals.
.PP
.nf
> more add_prdcr.sh
#!/bin/bash

echo "prdcr_add name=vm1_2 host=vm1 type=active xprt=sock port=60001 interval=20000000"
echo "prdcr_start name=vm1_2"
echo "prdcr_add name=vm1_1 host=vm1 type=active xprt=sock port=60000 interval=20000000"
echo "prdcr_start name=vm1_1"
echo "updtr_add name=policy2_h1 interval=2000000 offset=0"
echo "updtr_prdcr_add name=policy2_h1 regex=vm1_1"
echo "updtr_start name=policy2_h1"
echo "updtr_add name=policy5_h2 interval=5000000 offset=0"
echo "updtr_prdcr_add name=policy5_h2 regex=vm1_2"
echo "updtr_start name=policy5_h2"
.fi

.SS Invoke the script from an encompassing script:
.PP
In an encompassing script, use the following to pass the add_prdcr.sh script to the ldmsd_controller:
.nf
cmd="ldmsd_controller --host vm1 --port=61002"
$cmd --script "<<path_to_script>>/add_prdcr.sh"
.fi

.PP
Notes:
.IP \[bu]
See the ldmsd_controller man page for more description of its options.
.IP \[bu]
There is no requirement that aggregator intervals match collection intervals
.IP \[bu]
Because the collection and aggregation processes operate asynchronously there is the potential for duplicate data collection as well as missed samples.
The first is handled by the storage plugins by comparing generation numbers and not storing duplicates. The second implies either a loss
in fidelity (if collecting counter data) or a loss of data points here and there (if collecting differences of counter values or non counter
values). This can be handled using the synchronous option on both collector and aggregator but is not covered here.
.fi

.PP
At this point the ldmsd collector should be checked using the utility
.B ldms_ls
(See Using ldms_ls below). In this case you should see metric sets for both vm1_1 and vm1_2 displayed when you query the aggregator ldmsd using ldms_ls.


.SH CONFIGURE A STORE_CSV STORAGE PLUGIN USING LMDSD_CONTROLLER
Configure as ldmsd aggregator on a host that has access to a storage device using "sock" as the listening transport.

.SS Configure an aggregator
Configure an Aggregator as above.

.SS Set environment variables
.nf
.RS
export LD_LIBRARY_PATH=/tmp/opt/ovis/lib/:/tmp/opt/ovis/lib/ovis-ldms/:<path to libevent-2.0>/lib:$LD_LIBRARY_PATH
export ZAP_LIBPATH=/tmp/opt/ovis/lib/ovis-ldms
export LDMSD_PLUGIN_LIBPATH=/tmp/opt/ovis/lib/ovis-ldms
export PATH=/tmp/opt/ovis/sbin/:/tmp/opt/ovis/bin:$PATH
export LDMSD_SOCKPATH=/tmp/run/ldmsd  #(non-root. Change -S arguments accordingly)
.RE
.ni

.SS Write a script to add and start stores:
.PP
.nf
> more add_store.sh
#!/bin/bash

# whole path must exist
STORE_PATH=/XXX/ldmstest/store
mkdir -p $STORE_PATH
sleep 1

echo "load name=store_csv"
echo "config name=store_csv path=$STORE_PATH action=init altheader=0 rollover=30 rolltype=1"
echo "strgp_add name=policy_mem plugin=store_csv container=csv schema=meminfo"
echo "strgp_prdcr_add name=policy_mem regex=vm*"
echo "strgp_start name=policy_vmstat"
echo "strgp_add name=policy_vmstat plugin=store_csv container=csv schema=vmstat"
echo "strgp_prdcr_add name=policy_vmstat regex=vm*"
echo "strgp_start name=policy_vmstat"
.ni

.SS Invoke the script from an encompassing script:
.PP
.nf
cmd="ldmsd_controller --host vm1 --port=61002"
$cmd --script "<<path_to_script>>/add_store.sh"
.fi


.SS Verify the store
Go to data store and verify files have been created and are being written to
.nf
.RS
cd ~/stored_data/node/<container>
ls -ltr
.RE
.fi
You can now utilize this data.
.PP
Data will flush to the store when the OS flushes data unless an advanced flag is used. Thus,
in a default configuration, if you have a small number of nodes and/or a long interval,
you may not see data appear in the store for a few minutes.

.SS Notes:
.IP \[bu]
If you want to collect on a host and store that data on the same host, run two ldmsd's: one with a collector plugin only and one as an aggegrator with a store plugin only.
.PP

.SH USING LDMS_LS TO DISPLAY SETS/METRICS FROM AN LDMSD

.SS Set environment variables
.nf
.RS
export LD_LIBRARY_PATH=/tmp/opt/ovis/lib/:/tmp/opt/ovis/lib/ovis-ldms/:<path to libevent-2.0>/lib:$LD_LIBRARY_PATH
export ZAP_LIBPATH=/tmp/opt/ovis/lib/ovis-ldms
export LDMSD_PLUGIN_LIBPATH=/tmp/opt/ovis/lib/ovis-ldms
export PATH=/tmp/opt/ovis/sbin/:/tmp/opt/ovis/bin:$PATH
(LDMSD_SOCKPATH does not need to be set)
.RE
.ni

.SS Examples for running ldms_ls
.IP \[bu]
Query ldmsd on host vm1 listening on port 60000 using the sock transport for metric sets being served by that ldmsd
.nf
.RS
ldms_ls -h vm1 -x sock -p 60000
Should return:
vm1_1/meminfo
vm1_1/vmstat
.RE
.fi

.IP \[bu]
Query ldmsd on host vm1 listening on port 60000 using the sock transport for the names and contents of metric sets being served by that ldmsd.
Should return: Set names (vm1_1/meminfo and vm1_1/vmstat in this case) as well as all names and values associated with each set respectively.
Only vm1_1/meminfo shown here.
.nf
.RS
> ldms_ls -h vm1 -x sock-p 60000 -l
vm1_1/meminfo: consistent, last update: Wed Jul 31 21:51:08 2013 [246540us]
U64 33084652         MemTotal
U64 32092964         MemFree
U64 0                Buffers
U64 49244            Cached
U64 0                SwapCached
U64 13536            Active
U64 39844            Inactive
U64 5664             Active(anon)
U64 13540            Inactive(anon)
U64 7872             Active(file)
U64 26304            Inactive(file)
U64 2996             Unevictable
U64 2988             Mlocked
U64 0                SwapTotal
U64 0                SwapFree
U64 0                Dirty
U64 0                Writeback
U64 7164             AnonPages
U64 6324             Mapped
U64 12544            Shmem
U64 84576            Slab
U64 3948             SReclaimable
U64 80628            SUnreclaim
U64 1608             KernelStack
U64 804              PageTables
U64 0                NFS_Unstable
U64 0                Bounce
U64 0                WritebackTmp
U64 16542324         CommitLimit
U64 73764            Committed_AS
U64 34359738367      VmallocTotal
U64 3467004          VmallocUsed
U64 34356268363      VmallocChunk
U64 0                HugePages_Total
U64 0                HugePages_Free
U64 0                HugePages_Rsvd
U64 0                HugePages_Surp
U64 2048             Hugepagesize
U64 565248           DirectMap4k
U64 5726208          DirectMap2M
U64 27262976         DirectMap1G
.RE
.nf

.IP \[bu]
For a non-existent set
.nf
.RS
ldms_ls -h vm1 -x sock -p 60000 -l vm1_1/foo
ldms_ls: No such file or directory
ldms_ls: lookup failed for set 'vm1_1/foo'
.RE
.fi

.IP \[bu]
Display metadata about sets contained by vm1 ldmsd listening on port 60000
.nf
.RS
ldms_ls -h vm1 -x sock -p 60000 -v will output metadata information
.RE
.fi
.PP

.SH STOP AN LDMSD
.SS
To kill all ldmsd on a host
.nf
.RS
killall ldmsd
.RE
.ni

.SH PROTECTION DOMAIN TAGS (Cray XE/XK)
If you are going to be using the "ugni" transport (RDMA over Gemini) you will need to run with either system (as root) or user (as user) ptags. While root CAN run using any ptag the fact that its use is unknown to ALPS could cause collisions with applications.

.SS To see current ptags:
.nf
.RS
> apstat -P
PDomainID           Type    Uid   PTag     Cookie
LDMS              system      0     84 0xa9380000
.RE
.ni

.SS To create a userspace ptag:
.nf
.RS
apmgr pdomain -c <somenamehere>

Example:
> apmgr pdomain -c foo
> apstat -P
PDomainID           Type    Uid   PTag     Cookie
LDMS              system      0     84 0xa9380000
foo                 user     12345  233 0xa1230000
.RE
.fi
Note: A system administrator will have to setup system ptags and/or enable users to set up ptags.

.SS To remove a userspace ptag:
.nf
.RS
apmgr pdomain -r <somenamehere>
.RE
.fi
Note: The userid of the ptag being removed must match that of the user running the command or root

.SS PTAG-Related Enviroment variables for ldms (XE/XK)
Set the following environment variables for either user or system ptags (example shows user ptag values):
.nf
.RS
export LDMS_UGNI_PTAG 233
export LDMS_UGNI_COOKIE 0xa1230000
.RE
.fi

.SS Starting ldms from aprun with ptags
When running with user space ptags you must specify the ptag name when using aprun
.nf
.RS
aprun <<usual aprun args here>> -p foo ldmsd <<usual ldmsd flags here>>
or
aprun <<usual aprun args here>> -p foo ldms_ls <<usual ldms_ls flags here>>
.RE
.fi
Note: On some systems you will run aprun after a qsub -I or within a script specified in qsub or similiar.


.SH PROTECTION DOMAIN TAGS (Cray XC)
If you are going to be using the "ugni" transport (RDMA over Aries) you will need to run with either system (as root) or user (as user) ptags. While root CAN run using any ptag the fact that its use is unknown to ALPS could cause collisions with applications.

.SS To see current ptags:
.nf
.RS
> apstat -P
PDomainID   Type   Uid     Cookie    Cookie2
LDMS      system     0 0x86b80000          0
.RE
.ni

.SS To create a userspace ptag:
.nf
.RS
apmgr pdomain -c <somenamehere>

Example:
> apmgr pdomain -c foo
> apstat -P
PDomainID   Type   Uid     Cookie    Cookie2
LDMS      system     0 0x86b80000          0
foo         user 20596 0x86bb0000 0x86bc0000
.RE
.fi
Note: A system administrator will have to setup system ptags and/or enable users to set up ptags.

.SS To remove a userspace ptag:
.nf
.RS
apmgr pdomain -r <somenamehere>
.RE
.fi
Note: The userid of the ptag being removed must match that of the user running the command or root

.SS PTAG-Related Enviroment variables for ldms (XC)
Set the following environment variables.
On XC the ptag value doesn't matter but LDMS_UGNI_PTAG must be defined.
Set the Cookie (not Cookie2) for either user or system ptag.
.nf
.RS
export LDMS_UGNI_PTAG=0
export LDMS_UGNI_COOKIE=0x86bb0000
.RE
.fi

.SS Starting ldms from aprun with ptags
When running with user space ptags you must specify the ptag name when using aprun
.nf
.RS
aprun <<usual aprun args here>> -p foo ldmsd <<usual ldmsd flags here>>
or
aprun <<usual aprun args here>> -p foo ldms_ls <<usual ldms_ls flags here>>
.RE
.fi
Note: On some systems you will run aprun after a qsub -I or within a script specified in qsub or similiar.

.SH TROUBLESHOOTING

.SS What causes the following error: libibverbs: Warning: RLIMIT_MEMLOCK is 32768 bytes?
Running as a user with "max locked memory" set too low.
The following is an example of trying to run ldms_ls as a user with "max locked memory" set to 32k:
.nf
.RS
ldms_ls -h <hostname> -x rdma -p <portnum>
libibverbs: Warning: RLIMIT_MEMLOCK is 32768 bytes.
   This will severely limit memory registrations.
RDMA: recv_buf reg_mr failed: error 12
ldms_ls: Cannot allocate memory
.RE
.ni

.SS Why doesn't my ldmsd start?
.PP
Possible options:
.IP \[bu]
Check for existing /var/run/ldms/metric_socket or similar. Sockets can be left if an ldmsd did not clean up upon termination. kill -9 may leave the socket hanging around.
.IP \[bu]
The port you are trying to use may already be in use on the node. The following shows the logfile output of such a case:
.nf
.RS
Tue Sep 24 08:36:54 2013: Started LDMS Daemon version 2.1.0
Tue Sep 24 08:36:54 2013: Process 123456 listening on transport ugni:60020
Tue Sep 24 08:36:54 2013: EV_WARN: Can't change condition callbacks once they have been initialized.
Tue Sep 24 08:36:54 2013: Error 12 listening on the 'ugni' transport.
Tue Sep 24 08:36:54 2013: LDMS Daemon exiting...status 7
.RE
.ni
.IP \[bu]
If using the -l flag make sure that your log directory exists prior to running
.IP \[bu]
If writing to a store with this particular lmdsd make sure that your store directory exists prior to running
.IP \[bu]
If you are running on a Cray with transport ugni using a user space PTag, check that you called aprun with the -p flag
.RS
aprun -N 1 -n <number of nodes> -p <ptag name> run_my_ldmsd.sh
.RE
.RE

.SS How can I find what process is using the port?
.RS
netstat -abno
.RE

.SS Why arent all my hosts/sets adding to the aggregator?
Possible options:
.IP \[bu]
Running multiples on the same host from a script. Sometimes multiple ldmsctls running concurrently may collide in creating ports. They should clean up after themselves and this usually isn't an issue.
.IP \[bu]
use -m flag on the aggregator to use more memory when adding a lot of hosts
.IP \[bu]
use -p on the aggregator to use more processors
.SE

.SS Why isn't my ldmsd storing its own set to the store?
Currently, this is not supported. You can use a separate ldmsd on the same host to gather another ldmsd's data for that host.

.SS Why is my aggregator not responding (CRAY XE/XK)?
Running a ldmsd aggregator as a user but trying to aggregate from a ldmsd that uses a system ptag can result in the aggregator hanging (alive but not responding and not writing to the store). The following is the logfile output of such an aggregator:
.nf
.RS
Tue Sep 24 08:42:40 2013: Connected to host 'nid00081:60020'
Tue Sep 24 08:42:42 2013: cq_thread_proc: Error 11  monitoring the CQ.
.RE
.fi

.SH MAN PAGES
ldms comes with man pages. In the build process these will be installed in <build_path>/ovis/share/man.
Man pages are in the following catagories:
.SS General
General pages address information, such as ldms_build_install, ldms_quickstart, and ldms_authentication.
.SS Utilities
Utilities pages address the various utilities and commands such as ldmsd, ldmsd_controller, ldms_ls, and ldmsctl.
.SS Plugins
Plugin pages address all plugins, both samplers and stores. Naming convention for these pages is Plugin_XXX.
For example: Plugin_aries_mmr, Plugin_cray_system_sampler_variants, Plugin_kgnilnd, Plugin_meminfo, Plugin_procinterrupts, Plugin_procnetdev, Plugin_procnfs, Plugin_store_csv, Plugin_store_derived_csv, Plugin_store_sos, and Plugin_vmstat.

.SH NOTES
As part of the install, test scripts are placed in /tmp/opt/ovis/bin. These scripts
may serve as additional examples. These are being converted from using
the ldmsctl tool to the ldmsd_controller tool, so they may not be
fully updated at any given time.

.SH BUGS
No known bugs.


.SH SEE ALSO
ldms_build_install(7), ldmsd(8), ldmsd_controller(8), ldms_authentication(7), ldmsctl(8), ldms_build_install(7), ldms_ls(8), Plugin_aries_mmr(7), Plugin_cray_system_sampler_variants(7), Plugin_kgnilnd(7), Plugin_meminfo(7), Plugin_procinterrupts(7), Plugin_procnetdev(7), Plugin_procnfs(7), Plugin_store_csv(7), Plugin_store_derived_csv(7), Plugin_store_sos(7), Plugin_vmstat(7)

