# run many daemons under slurm. assumes exclusive use of nodes.
# use of fabric or rdma plugins will require allowroot=1 in the environment.
# control of network selection is in sbatch.cluster.
# export LDMSNET=[sock,fabric,sockfabric,rdma] to control network selection
# before launching pll-ldms-static-test.sh.
portbase=11000
ulimit -n 100000
MESSAGE starting l1, l2 aggs and collectors
VGARGS="--tool=drd --time-stamp=yes --gen-suppressions=all --trace-mutex=yes"
VGARGS="--track-origins=yes --leak-check=full --show-leak-kinds=definite --time-stamp=yes --gen-suppressions=all  --main-stacksize=256000000"
if test -f $pkglibdir/ldms.supp; then
	VGARGS="$VGARGS --suppressions=$pkglibdir/ldms.supp"
fi
# load multithread grind
#LDMS_CPU_GRIND -n 600 -m multiply
SET_LOG_LEVEL INFO
# start collector(s)
if test "x$maxdaemon" = "x"; then
	maxdaemon=3
fi

DAEMONS $(seq 1 $maxdaemon)
LDMSD_EXTRA="-m 20k"
VGTAG=.samp
#netstat -tonp
# start n-2 sampler daemons
#vgon
LDMSD -s 1 -c $(seq 3 $maxdaemon)
vgoff

# without the next sleep, ldms_ls hits config parsing for some
# daemons and those hit assert
SLEEP 60
BARRIER_VERBOSE="-v"
echo STRTWAIT1 $(date +%s.%N)
llw_timeout=$(( $maxdaemon / 4 + 10 ))
LDMS_LS_WAIT meminfo $llw_timeout $(seq 3 $maxdaemon)
echo DONEWAIT1 LDMS_LS_WAIT $(date +%s.%N)

#timeout=20
# run/ldmsd.pid.$k for all k in argument list (after shifting off the timeout value)
# if fail, set bypass=1.
# info: $myproc is which daemon number this instance is
#CHECK_PIDS $timeout `seq 3 $maxdaemon` ; # function which waits until all pids files exist and all pids listed in pid files also exist;

ulimit -c unlimited
# scale m to handle maxdaemon
# -m > 450 or so seems to be incompatible with arm/valgrind 3.13
LDMSD_EXTRA="-m 32G"
vgon
VGTAG=.L2
# start L2 on daemon 1
LDMSD 1
vgoff
#vgon
VGTAG=.L1
# start L1 on daemon 2, with sampler daemons 3..maxdaemon
LDMSD -s 3000000 -P pll.producer,`seq -s, 3 $maxdaemon` 2
vgoff
SLEEP 30
FILECNT_LDMSD $ALL_DAEMONS
#vgon
LDMS_LS 1
vgoff

# no cores from samplers.
ulimit -c 0

# this loop takes about 5 minutes to allow for barriers to clear time_waits
flap=0
while test $(SEC_LEFT) -gt 600; do
	if test $bypass = "1"; then
		break
	fi
	((flap++))
	MESSAGE "FLAP $flap starting at $(date +%s)"
	BEST_DELAY 300
	# check that 3-n started sampling and data reached L2
	LDMS_AGG_WAIT 1 30 meminfo 30 $(seq 3 $maxdaemon)
	echo DONEWAIT2.$flap LDMS_AGG_WAIT $(date +%s.%N)
	LDMS_STATUS 1
	BEST_DELAY 300
	WAIT_ALL 0 $ALL_DAEMONS ; # this should be redundant and fast
	# stop all samplers and restart a few cycles
	KILL_LDMSD $(seq 3 $maxdaemon)
	BEST_DELAY 300
	WAIT_ALL 0 $ALL_DAEMONS
	echo DONEWAIT3.$flap KILL_LDMSD $(date +%s.%N)
	# check that 3-n samplers got dropped by aggregator
	# still the local dstat set should be seen
	LDMS_STATUS 2

	BEST_DELAY 300
        lawsc_agg=1
        lawsc_agg_connect_timeout=5
        lawsc_count_target=1
        lawsc_count_timeout=260
	LDMS_AGG_WAIT_SET_COUNT \
		$lawsc_agg \
		$lawsc_agg_connect_timeout \
		$lawsc_count_target \
		$lawsc_count_timeout

	echo DONEWAIT4.$flap LDMS_AGG_WAIT_SET_COUNT $(date +%s.%N)
	# we should now have just dstat on the l1, l2
	SLEEP 5
	FILECNT_LDMSD $ALL_DAEMONS
	# restart samplers.
	LDMSD -s 1 -c $(seq 3 $maxdaemon)
done


#vgon
LDMS_LS 2
#vgoff
# kill l2
KILL_LDMSD 1
SLEEP 1
WAIT_ALL 0 $ALL_DAEMONS
# kill l1, samps
KILL_LDMSD $(seq 3 $maxdaemon) 2
BEST_DELAY 300
WAIT_ALL 0 $ALL_DAEMONS
LDMS_CPU_GRIND stop
MESSAGE logs and data under ${TESTDIR}
# if configured, reset ownership of output
if test "x$USER" = "xroot"; then
	if test -n "$CHOWN_USER"; then
		chown -R $CHOWN_USER.$CHOWN_USER $TESTDIR
	fi
fi
