# run many daemons under slurm. assumes exclusive use of nodes.
# use of fabric or rdma plugins will require allowroot=1 in the environment.
# control of network selection is in sbatch.cluster.
# export LDMSNET=[sock,fabric,sockfabric,rdma] to control network selection
# before launching pll-ldms-static-test.sh.
portbase=11000
ulimit -n 100000
MESSAGE starting l1, l2 aggs and collectors
VGARGS="--tool=drd --time-stamp=yes --gen-suppressions=all --track-origins=yes"
VGARGS="--track-origins=yes --leak-check=full --show-leak-kinds=definite --time-stamp=yes --gen-suppressions=all  --main-stacksize=256000000"
if test -f $pkglibdir/ldms.supp; then
	VGARGS="$VGARGS --suppressions=$pkglibdir/ldms.supp"
fi
usevg_L2=1
usevg_L1=0
usevg_L0=0
usevg_ls=0
# load multithread grind
#LDMS_CPU_GRIND -n 600 -m multiply
SET_LOG_LEVEL INFO
# start collector(s)
if test "x$maxdaemon" = "x"; then
	maxdaemon=3
fi

DAEMONS $(seq 1 $maxdaemon)
LDMSD_EXTRA="-m 20k"
VGTAG=.samp
#netstat -tonp
# start n-2 sampler daemons
if test $usevg_L0 = "1"; then
	vgon
fi
LDMSD -s 1 -c $(seq 3 $maxdaemon)
vgoff

# without the next sleep, ldms_ls hits config parsing for some
# daemons and those hit assert
SLEEP 60
#BARRIER_VERBOSE="-v"
echo STRTWAIT1 $(date +%s.%N)
llw_timeout=$(( $maxdaemon / 4 + 10 ))
LDMS_LS_WAIT meminfo $llw_timeout $(seq 3 $maxdaemon)
echo DONEWAIT1 LDMS_LS_WAIT $(date +%s.%N)

# scale m to handle maxdaemon
# -m > 450 or so seems to be incompatible with arm/valgrind 3.13
LDMSD_EXTRA="-m 40g"
if test $usevg_L2 = "1"; then
	vgon
fi
VGTAG=.L2
# start L2 on daemon 1
LDMSD 1
vgoff

# start L1 on daemon 2, with sampler daemons 3..maxdaemon
if test $usevg_L2 = "1"; then
	vgon
fi
VGTAG=.L1
LDMSD -a -s 3000000 -P pll.producer,`seq -s, 3 $maxdaemon` 2
vgoff
SLEEP 30

FILECNT_LDMSD $ALL_DAEMONS

if test $usevg_ls = "1"; then
	vgon
fi
LDMS_LS 1
vgoff

# enable core dumps on aggregators
ulimit -c unlimited

# this loop takes about 5 minutes to allow for barriers to clear time_waits
while test $(SEC_LEFT) -gt 1200; do
	if test $bypass = "1"; then
		break
	fi
	MESSAGE "FLAP $flap starting at $(date +%s) $(date)"
	BEST_DELAY 400
	# check that 3-n started sampling and data reached L2
	LDMS_AGG_WAIT 1 30 meminfo 30 $(seq 3 $maxdaemon)
	FILECNT_LDMSD $ALL_DAEMONS
	echo DONEWAIT2.$flap LDMS_AGG_WAIT $(date +%s.%N)
	BEST_DELAY 400
	WAIT_ALL 0 $ALL_DAEMONS ; # this should be redundant and fast
	# stop L1 agg
	MESSAGE Stopping L1
	KILL_LDMSD 2
	BEST_DELAY 400
	WAIT_ALL 0 $ALL_DAEMONS
	echo DONEWAIT3.$flap after KILL_LDMSD 2 $(date +%s.%N)
	# check that 3-n samplers got dropped by aggregator
	# still the local dstat set should be seen

	BEST_DELAY 400
        lawsc_agg=1
        lawsc_agg_connect_timeout=5
        lawsc_count_target=1
        lawsc_count_timeout=400
	LDMS_AGG_WAIT_SET_COUNT \
		$lawsc_agg \
		$lawsc_agg_connect_timeout \
		$lawsc_count_target \
		$lawsc_count_timeout

	echo DONEWAIT4.$flap LDMS_AGG_WAIT_SET_COUNT $(date +%s.%N)
	# we should now have just dstat on the l1, l2
	# restart L1
	MESSAGE restarting L1
	if test $usevg_L2 = "1"; then
		vgon
	fi
	LDMSD -a -s 4000000 -P pll.producer,`seq -s, 3 $maxdaemon` 2
	vgoff
	SLEEP 60
done
FILECNT_LDMSD $ALL_DAEMONS

LDMS_LS 2
# kill l2
KILL_LDMSD 1
SLEEP 1
WAIT_ALL $(seq $maxdaemon)
# kill l1, samps
KILL_LDMSD $(seq 3 $maxdaemon) 2
BEST_DELAY 300
WAIT_ALL $(seq $maxdaemon)
#LDMS_CPU_GRIND stop
MESSAGE logs and data under ${TESTDIR}
# if configured, reset ownership of output
if test "x$USER" = "xroot"; then
	if test -n "$CHOWN_USER"; then
		chown -R $CHOWN_USER.$CHOWN_USER $TESTDIR
	fi
fi
