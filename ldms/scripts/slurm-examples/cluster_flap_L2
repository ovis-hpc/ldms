# run many daemons under slurm. assumes exclusive use of nodes.
# use of fabric or rdma plugins will require allowroot=1 in the environment.
# control of network selection is in sbatch.cluster.
# export LDMSNET=[sock,fabric,sockfabric,rdma] to control network selection
# before launching pll-ldms-static-test.sh.
portbase=11000
ulimit -n 100000
MESSAGE starting l1, l2 aggs and collectors
VGARGS="--tool=drd --time-stamp=yes --gen-suppressions=all"
VGARGS="--track-origins=yes --leak-check=full --show-leak-kinds=definite --time-stamp=yes --gen-suppressions=all  --main-stacksize=256000000"
if test -f $pkglibdir/ldms.supp; then
	VGARGS="$VGARGS --suppressions=$pkglibdir/ldms.supp"
fi
usevg_L2=1
usevg_L1=0
usevg_L0=0
usevg_ls=0
# load multithread grind
SET_LOG_LEVEL INFO
# start collector(s)
if test "x$maxdaemon" = "x"; then
	maxdaemon=3
fi

DAEMONS $(seq 1 $maxdaemon)
LDMSD_EXTRA="-m 20k"
VGTAG=.samp
#netstat -tonp
# start n-2 sampler daemons
if test $usevg_L0 = "1"; then
	vgon
fi
LDMSD -s 1 -c $(seq 3 $maxdaemon)
vgoff

# without the next sleep, ldms_ls_wait hits config parsing for some
# daemons and those hit assert; remove this sleep when test cluster_bug runs
# w/out errors.
SLEEP 60
#BARRIER_VERBOSE="-v"
echo STRTWAIT1 $(date +%s.%N)
llw_timeout=$(( $maxdaemon / 4 + 10 ))
LDMS_LS_WAIT meminfo $llw_timeout $(seq 3 $maxdaemon)
echo DONEWAIT1 LDMS_LS_WAIT $(date +%s.%N)

# scale m to handle maxdaemon
# -m > 450 or so seems to be incompatible with arm/valgrind 3.13
LDMSD_EXTRA="-m 64M"
if test $usevg_L2 = "1"; then
	vgon
fi
VGTAG=.L2
# start L2 on daemon 1
LDMSD 1
vgoff

# start L1 on daemon 2, with sampler daemons 3..maxdaemon
if test $usevg_L2 = "1"; then
	vgon
fi
VGTAG=.L1
LDMSD -s 3000000 -P pll.producer,`seq -s, 3 $maxdaemon` 2
vgoff
SLEEP 30

FILECNT_LDMSD $ALL_DAEMONS

if test $usevg_ls = "1"; then
	vgon
fi
LDMS_LS 1
vgoff

# this loop takes about 5 minutes to allow for barriers to clear time_waits
for flap in $(seq 3); do
	if test $bypass = "1"; then
		break
	fi
	MESSAGE "FLAP $flap starting at $(date +%s) $(date)"
	BEST_DELAY 300
	# check that 3-n started sampling and data reached L2
	LDMS_AGG_WAIT 1 30 meminfo 30 $(seq 3 $maxdaemon)
	FILECNT_LDMSD $ALL_DAEMONS
	echo DONEWAIT2.$flap LDMS_AGG_WAIT $(date +%s.%N)
	BEST_DELAY 300
	WAIT_ALL 0 $ALL_DAEMONS ; # this should be redundant and fast
	# stop L1 agg
	MESSAGE Stopping L2
	KILL_LDMSD 1
	BEST_DELAY 300
	WAIT_ALL 0 $ALL_DAEMONS
	echo DONEWAIT3.$flap KILL_LDMSD $(date +%s.%N)

	# TODO: check that L2 subscribers got dropped at L1
	BEST_DELAY 300
        lawsc_agg=2
        lawsc_agg_connect_timeout=5
        lawsc_count_target=0
        lawsc_count_timeout=260
	LDMS_AGG_WAIT_SET_COUNT \
		$lawsc_agg \
		$lawsc_agg_connect_timeout \
		$lawsc_count_target \
		$lawsc_count_timeout

	echo DONEWAIT4.$flap LDMS_AGG_WAIT_SET_COUNT $(date +%s.%N)
	# we should now have just dstat on the l1, l2
	# restart L1
	MESSAGE restarting L2
	if test $usevg_L2 = "1"; then
		vgon
	fi
	LDMSD 1
	vgoff
	SLEEP 60
done
FILECNT_LDMSD $ALL_DAEMONS

LDMS_LS 2
# kill l2
KILL_LDMSD 1
SLEEP 1
WAIT_ALL $(seq $maxdaemon)
# kill l1, samps
KILL_LDMSD $(seq 3 $maxdaemon) 2
BEST_DELAY 300
WAIT_ALL $(seq $maxdaemon)
MESSAGE logs and data under ${TESTDIR}
# if configured, reset ownership of output
if test "x$USER" = "xroot"; then
	if test -n "$CHOWN_USER"; then
		chown -R $CHOWN_USER.$CHOWN_USER $TESTDIR
	fi
fi
