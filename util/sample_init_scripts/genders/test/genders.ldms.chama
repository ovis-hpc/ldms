# ldms genders by node selfreported hostname needed for init.d control

# switch these to INFO or DEBUG for short periods or by node type.
chama[1-1232],chama-gw[1-48],chama-lsm[1-8],chama-rps[1-8] ldmsd_dbg=ERROR
chama[1-1232],chama-gw[1-48],chama-lsm[1-8],chama-rps[1-8] ldmsaggd_dbg=ERROR

# monitoring data collectors are ldmsd; what about iscb[1-140],sw[1-32]?
chama[1-1232],chama-gw[1-48],chama-lsm[1-8],chama-rps[1-8] ldmsd
# define transport for data collectors
chama[1-1232],chama-gw[1-48],chama-lsm[1-8],chama-rps[1-8] ldmsd_port=411,ldmsd_xprt=rdma
# define the hostname needed for the transport.
chama[1-1232],chama-gw[1-48],chama-lsm[1-8],chama-rps[1-8] ldmsd_host=%n-ib0
# define sampling schedule (microsecond units). Is 10 sec too fast/slow?
chama[1-1232],chama-gw[1-48],chama-lsm[1-8],chama-rps[1-8] ldmsd_interval_default=10000000,ldmsd_offset_default=0
# what to collect
chama[1-1232],chama-gw[1-48],chama-lsm[1-8],chama-rps[1-8] slurmjobid:meminfo:vmstat:procnfs:lustre2_client:procstatutil2:procnetdev:sysclassib
# OPTIONS for specific plugins or host sets
# what about fscratch, gscratch in lustre2_client?
chama[1-1232],chama-gw[1-48],chama-lsm[1-8],chama-rps[1-8] ldmsd_meminfo=with_jobid/1
chama[1-1232],chama-gw[1-48],chama-lsm[1-8],chama-rps[1-8] ldmsd_vmstat=with_jobid/1
chama[1-1232],chama-gw[1-48],chama-lsm[1-8],chama-rps[1-8] ldmsd_procnfs=with_jobid/1
# lustre hasn't been updated to track slurm jobid yet.
chama[1-1232],chama-gw[1-48],chama-lsm[1-8],chama-rps[1-8] ldmsd_lustre2_client=llite/scratch1
# change maxcpu to 32 if hyperthreading is on or optional per-user.
chama[1-1232],chama-gw[1-48],chama-lsm[1-8],chama-rps[1-8] ldmsd_procstatutil2=with_jobid/1:maxcpu/16
# the net data has holes in the gw for eth2,eth3 and in rest of nodes for ib1.
chama[1-1232],chama-lsm[1-8],chama-rps[1-8],chama-gw[1-48] ldmsd_procnetdev=with_jobid/1:ifaces/eth0&eth1&eth2&eth3&ib0&ib1
# verify correct ib ports for ldmsd_sysclassib
chama[1-1232],chama-gw[1-48],chama-lsm[1-8],chama-rps[1-8] ldmsd_sysclassib=with_jobid/1:ports/mlx4_0.1
# qlogic cards?
#chama[1-1232],chama-gw[1-48],chama-lsm[1-8],chama-rps[1-8] ldmsd_sysclassib=with_jobid/1:ports/qib0.1

# component id groups
chama[1-1232] ldmsd_idbase=0
chama-rps[1-8] ldmsd_idbase=11000
chama-lsm[1-8] ldmsd_idbase=12000
chama-gw[1-48] ldmsd_idbase=13000

# aggregator 1st level
# CLIENTOFLIST is expanded to match ldmsd_clientof values.
# bOOTNODELIST expands to bootnode values. See man page for alternatives.
# Port 411 cannot be used for both ldms_aggd and ldmsd if xprt is the same.
#
# %n in ldmsagdd means aggregate yourself; could shove off to aries.
chama-rps[1-8] ldmsaggd=BOOTNODELIST,%n
# transport
chama-rps[1-8] ldmsaggd_port=411,ldmsaggd_xprt=sock
chama-rps[1-8] ldmsaggd_host=%n
# who aggregators will  report to
chama-rps[1-8] ldmsaggd_clientof=aries
# schedule aggregation a delta T after collection.
chama-rps[1-8] ldmsaggd_interval_default=10000000,ldmsaggd_offset_default=350000

# aries is agging ataco 1st level aggregator
aries ldmsaggd=AGGCLIENTOFLIST
# aries agg schedule (offset should be bigger than ataco agg offset.
aries ldmsaggd_interval_default=10000000,ldmsaggd_offset_default=750000
# store format
aries ldmsaggd_stores=store_csv:store_derived_csv
aries ldmsaggd_dbg=DEBUG
aries ldmsaggd_store_csv=altheader/1:id_pos/1:rolltype/2:rollover/0
# yes, need the // in the next line; 1st / is the = replacement, remember?
# oh, and file may be missing at the moment on aries.
# oh, and need path treated as its own gender and maybe as a template string.
aries ldmsaggd_store_derived_csv=altheader/1:id_pos/1:rolltype/2:rollover/0:derivedconf//projects_srn/ovis/aries/Config/store.config:agesec/40
